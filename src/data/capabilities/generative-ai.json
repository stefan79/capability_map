{
  "id": "gai",
  "title": "Generative AI incl. Agentic AI",
  "children": [
    {
      "id": "aa",
      "title": "Agentic AI",
      "children": [
        {
          "id": "ard",
          "title": "Agent Registry & Discovery",
          "status": "not implemented",
          "type": "technology",
          "description": "An Agent Registry is a centralized service where AI agents can register their capabilities and be discovered by other agents. This enables dynamic and scalable collaboration between different agentic systems, much like a phone book for agents.",
          "link": "https://dev.to/sreeni5018/building-an-ai-agent-registry-server-with-fastapi-enabling-seamless-agent-discovery-via-a2a-15dj"
        },
        {
          "id": "apeaa",
          "title": "Agent Protocols e.g., A2A, ACP",
          "status": "not implemented",
          "type": "pattern",
          "description": "Agent-to-Agent (A2A) protocols define a standard way for AI agents, potentially built on different platforms, to communicate and interoperate. This standardization is crucial for creating complex systems where multiple specialized agents collaborate to complete tasks.",
          "link": "https://developers.googleblog.com/en/a2a-a-new-era-of-agent-interoperability/"
        },
        {
          "id": "ast",
          "title": "Agent SDK + templates",
          "status": "not implemented",
          "type": "technology",
          "description": "An Agent SDK (Software Development Kit) and templates provide developers with pre-built tools, libraries, and boilerplate code to accelerate the creation of AI agents. They simplify common tasks like connecting to LLMs, managing state, and defining tools.",
          "link": "https://ai-sdk.dev/docs/introduction"
        },
        {
          "id": "mrd",
          "title": "MCP Registry & Discovery",
          "status": "not implemented",
          "type": "technology",
          "description": "A Multi-Capability-Provider (MCP) Registry allows agents to discover and utilize a wide range of tools and services from different providers. This enables agents to dynamically extend their capabilities by finding and integrating the best available resources for a given task.",
          "link": "https://www.infoq.com/news/2025/06/secure-agent-discovery-ans/"
        },
        {
          "id": "mm",
          "title": "Memory Management",
          "status": "not implemented",
          "type": "technology",
          "description": "Memory management for AI agents involves systems for storing and retrieving information over time, enabling them to maintain context in long conversations and learn from past interactions. It includes both short-term (session) and long-term (persistent) memory.",
          "link": "https://www.ibm.com/think/topics/ai-agent-memory"
        }
      ]
    },
    {
      "id": "muf",
      "title": "Model Use & Finetuning",
      "children": [
        {
          "id": "fm",
          "title": "Foundation Models",
          "status": "implemented",
          "type": "technology",
          "description": "Foundation models are large-scale AI models trained on vast amounts of data, designed to be adapted to a wide range of downstream tasks. They serve as a powerful base for developing more specialized AI applications with minimal fine-tuning.",
          "link": "https://research.ibm.com/blog/what-are-foundation-models"
        },
        {
          "id": "mg",
          "title": "Model Gateway",
          "status": "not implemented",
          "type": "technology",
          "description": "An AI Model Gateway acts as a centralized control plane between applications and various AI models. It manages access, monitors usage, and ensures secure and efficient deployment of large language models (LLMs).",
          "link": "https://konghq.com/blog/enterprise/what-is-an-ai-gateway"
        },
        {
          "id": "dmr",
          "title": "Dynamic model routing",
          "status": "not implemented",
          "type": "pattern",
          "description": "Dynamic model routing is the process of intelligently directing incoming requests to the most appropriate AI model from a pool of available models. This routing can be based on factors like cost, performance, or the specific nature of the request, optimizing the overall system.",
          "link": "https://medium.com/@simsketch/model-routing-in-ai-getting-the-right-request-to-the-right-model-dd21bab7c129"
        },
        {
          "id": "lps",
          "title": "Libraries: PEFT, SFT, RFT...",
          "status": "not implemented",
          "type": "technology",
          "description": "Specialized libraries for fine-tuning large models, including Parameter-Efficient Fine-Tuning (PEFT), Supervised Fine-Tuning (SFT), and Reinforcement Fine-Tuning (RFT). These techniques adapt models for specific tasks efficiently.",
          "link": "https://github.com/huggingface/peft"
        },
        {
          "id": "sc",
          "title": "Semantic Cache",
          "status": "not implemented",
          "type": "technology",
          "description": "A semantic cache stores the results of previous, semantically similar queries to a large language model. When a new, similar query arrives, the cached result can be returned, reducing latency, cost, and redundant computations.",
          "link": "https://redis.io/blog/what-is-semantic-caching/"
        }
      ]
    },
    {
      "id": "de",
      "title": "Data & Embeddings",
      "children": [
        {
          "id": "c",
          "title": "Chunking",
          "status": "partially",
          "type": "pattern",
          "description": "Chunking is the process of breaking down large documents into smaller, semantically relevant pieces of text. This is a critical step in Retrieval-Augmented Generation (RAG) to ensure the most relevant context is provided to the language model.",
          "link": "https://stackoverflow.blog/2024/12/27/breaking-up-is-hard-to-do-chunking-in-rag-applications/"
        },
        {
          "id": "o",
          "title": "OCR",
          "status": "partially",
          "type": "technology",
          "description": "Optical Character Recognition (OCR) technology converts images of typed, handwritten, or printed text into machine-encoded text. It is essential for digitizing documents and making their content searchable and usable by AI systems.",
          "link": "https://aws.amazon.com/what-is/ocr/"
        },
        {
          "id": "vs",
          "title": "Vector Storage",
          "status": "partially",
          "type": "technology",
          "description": "Vector storage, or a vector database, is designed to store and query high-dimensional vector embeddings efficiently. It's a core component for AI applications like semantic search and RAG, enabling fast similarity searches.",
          "link": "https://www.pinecone.io/learn/vector-database/"
        },
        {
          "id": "are",
          "title": "Advanced RAG, e.g., reranking",
          "status": "not implemented",
          "type": "pattern",
          "description": "Advanced RAG techniques improve upon basic retrieval by adding steps like reranking, where a more sophisticated model re-orders retrieved documents to improve relevance. This enhances the quality of the context provided to the LLM.",
          "link": "https://developer.nvidia.com/blog/enhancing-rag-pipelines-with-re-ranking/"
        },
        {
          "id": "ar",
          "title": "Agentic RAG",
          "status": "not implemented",
          "type": "pattern",
          "description": "Agentic RAG combines AI agents with retrieval systems, allowing the agent to dynamically decide when and how to retrieve information to solve complex, multi-step problems. This enables more autonomous and intelligent information seeking.",
          "link": "https://developer.nvidia.com/blog/traditional-rag-vs-agentic-rag-why-ai-agents-need-dynamic-knowledge-to-get-smarter/"
        }
      ]
    },
    {
      "id": "l",
      "title": "LLMOps",
      "children": [
        {
          "id": "ot",
          "title": "Observability & Tracing",
          "status": "not implemented",
          "type": "policy",
          "description": "LLM observability involves monitoring and understanding the behavior of large language models in production. Tracing provides detailed insights into the lifecycle of a request, helping to debug issues and optimize performance.",
          "link": "https://www.akira.ai/insights/llm-observability-and-monitoring"
        },
        {
          "id": "eidc",
          "title": "Evaluations incl. dataset creation",
          "status": "not implemented",
          "type": "pattern",
          "description": "This involves creating high-quality datasets specifically for evaluating the performance of LLMs on various tasks. A robust evaluation set is crucial for assessing model accuracy, fairness, and safety before and after deployment.",
          "link": "https://kili-technology.com/large-language-models-llms/how-to-build-llm-evaluation-datasets-for-your-domain-specific-use-cases"
        },
        {
          "id": "por",
          "title": "Prompt Ops & Repository",
          "status": "not implemented",
          "type": "technology",
          "description": "Prompt Ops involves managing the entire lifecycle of prompts, from creation and testing to deployment and versioning. A prompt repository is a centralized system for storing, sharing, and collaborating on prompts across teams.",
          "link": "https://www.promptpanda.io/blog/prompt-repository/"
        },
        {
          "id": "po",
          "title": "Prompt Optimization",
          "status": "not implemented",
          "type": "pattern",
          "description": "Prompt optimization is the process of refining and improving prompts to elicit the best possible responses from a large language model. This can involve manual tuning or automated techniques to enhance clarity, specificity, and overall performance.",
          "link": "https://www.promptingguide.ai/guides/optimizing-prompts"
        }
      ]
    }
  ]
}

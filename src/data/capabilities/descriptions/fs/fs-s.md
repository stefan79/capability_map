## Capability Overview
	• Analytical AI capability encompasses the application of machine learning (ML) and operations research (OR) techniques to analyze data, identify patterns, and generate actionable insights.
	• It addresses the challenge of extracting predictive or prescriptive value from complex datasets to support decision-making processes.
	• This capability enables enterprises to optimize operations, forecast trends, and automate data-driven decisions without prescribing specific implementation methods.
	• It is distinct from generative or agentic AI by focusing on analytical modeling rather than content creation or autonomous action.

## Business Value & Supported Use Cases
	• Enables improved decision accuracy and operational efficiency through predictive analytics and optimization.
	• Supports use cases such as demand forecasting, risk assessment, resource allocation, fraud detection, and customer segmentation.
	• Relevant across AI adoption stages:
		- Experimentation: validating analytical models on pilot datasets.
		- Scale-out: deploying models across multiple business units or geographies.
		- Industrialization: integrating analytical models into core business processes.
		- Regulated production: ensuring compliance and auditability of analytical outputs in regulated industries.

## Maturity Expectations
	• Emerging: Analytical models are ad hoc, limited in scope, and lack integration with enterprise data sources.
	• Well-established: Models are standardized, reproducible, and integrated with operational systems; monitoring and retraining processes exist.
	• Strategic / Differentiating: Analytical AI drives competitive advantage through advanced optimization and real-time decision support embedded in business workflows.
	• Foundational: Analytical AI is a core component of enterprise data strategy, with governance, scalability, and lifecycle management fully institutionalized.
	• “Good” at scale means consistent model performance, automated lifecycle management, and alignment with business objectives.
	• Under-maturity signs include siloed models, lack of version control, poor data quality, and absence of performance monitoring.

## Mandatory vs Optional Usage
	• Mandatory when:
		- Analytical AI supports production-critical decisions impacting revenue, compliance, or safety.
		- Deployed at enterprise scale requiring consistent governance and operational reliability.
		- Used in regulated environments demanding audit trails and explainability.
	• Optional when:
		- Limited to proof-of-concept projects or isolated teams experimenting with data.
		- Supporting exploratory or non-critical use cases without direct business impact.
	• The classification reflects the need for robustness, traceability, and integration proportional to business risk and scale.

## Key Dependencies & Related Capabilities
	• Technical:
		- Reliable and governed data infrastructure.
		- Feature engineering and data preprocessing capabilities.
		- Model training, validation, and deployment pipelines.
	• Governance:
		- Data privacy and compliance frameworks.
		- Model risk management and explainability standards.
	• Operational:
		- Monitoring and incident management systems.
		- MLOps platforms for lifecycle automation.
	• Related capabilities:
		- Platform Operations & MLOps / LLMOps.
		- Governance, Risk & Compliance.
		- Generative AI (for hybrid analytical-generative workflows).

## Risks of Omission or Poor Implementation
	• Architectural risks include fragmented model development, inconsistent data usage, and lack of scalability.
	• Operational risks involve model drift, unmonitored performance degradation, and delayed incident response.
	• Compliance risks arise from insufficient auditability, lack of explainability, and failure to meet regulatory requirements.
	• Typical failure modes include siloed analytics efforts, uncontrolled model proliferation, and inability to demonstrate model effectiveness or fairness.

## Example Metrics & KPIs
	• Model accuracy, precision, recall, or other relevant performance metrics.
	• Percentage of analytical models deployed to production versus in development.
	• Time-to-deployment from model development to operational use.
	• Number of incidents related to model failures or data quality issues.
	• Compliance audit pass rates for model governance and documentation.
	• Resource utilization and cost efficiency of analytical AI infrastructure.
